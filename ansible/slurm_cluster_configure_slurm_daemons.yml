---

- name: Configure slurm cluster (slurm master and slurm workers)
  hosts:
    - slurm_master_group
    - slurm_compute_group
    - slurm_login_group
  gather_facts: true
  become: true
  remote_user: "{{ slurm_cluster_ssh_remote_user }}"

  tasks:

    - name: Common setup
      import_tasks: shared_tasks/common_setup.yml

    - name: Configure NFS mounts
      import_role:
        name: ome.nfs_mount
      when: "'slurm_nfs_clients_group' in group_names"
      tags: nfs

    - name: Add OpenHPC repositories
      yum:
        name: "{{ slurm_cluster_ohpc_repos_url }}"
        state: installed

    - name: Install OpenHPC runtime Slurm packages
      yum:
        name:
          - "slurm-ohpc"
          - "munge-ohpc"
          - "slurm-example-configs-ohpc"
        state: present

    - name: Ensure the Slurm spool directory exists
      file:
        path: /var/spool/slurm
        owner: slurm
        group: slurm
        mode: 0755
        state: directory

    - name: Ensure the Slurm log directory exists
      file:
        path: /var/log/slurm
        owner: slurm
        group: slurm
        mode: 0755
        state: directory

    - name: Generate a Munge key for the cluster (in slurm master machine)
      command: "dd if=/dev/urandom of=/etc/munge/munge.key bs=1 count=1024"
      args:
        creates: "/etc/munge/munge.key"
      when: "'slurm_master_group' in group_names"

    - name: Retrieve Munge key from Slurm master host
      slurp:
        src: "/etc/munge/munge.key"
      register: _slurm_munge_key
      when: "'slurm_master_group' in group_names"

    - name: Write Munge key to every machine
      copy:
        content: "{{ hostvars[groups.slurm_master_group[0]]['_slurm_munge_key']['content'] | b64decode }}"
        dest: "/etc/munge/munge.key"
        owner: munge
        group: munge
        mode: 0400
      notify:
        - Restart Munge service

    - name: Start and enable munge service
      service:
        name: munge
        state: started
        enabled: true

    - name: Apply customised SLURM configuration
      template:
        src: slurm.conf.j2
        dest: /etc/slurm/slurm.conf
        owner: root
        group: root
        mode: 0644
      notify: Restart SLURM service
      tags: slurm_config

    - name: Create local /scratch folder with permission 777
      file:
        path: /scratch
        state: directory
        mode: 0777
        owner: root
        group: root

    # - name: Create the local accounts defined in the portal
    #   import_role:
    #     name: ansible-role-dcc-portal-accounts


    - name: Install and configure slurm accounting daemon
      block:

          - name: install mariadb-server rpm
            yum:
              name: mariadb-server
              state: installed

          - name: Enable and start mariadb server
            service:
              name: mariadb
              state: started
              enabled: true

          - name: install MySQL-python rpm (required by ansible)
            yum:
              name: MySQL-python
              state: installed

          - name: create mysql db for accounting
            mysql_db:
              name: "{{ slurm_cluster_slurmdbd_mysql_db }}"
              state: present

          - name: create mysql account for slurm accounting daemon
            mysql_user:
              name: "{{ slurm_cluster_slurmdbd_mysql_user }}"
              password: "{{ slurm_cluster_slurmdbd_mysql_password }}"
              priv: '{{ slurm_cluster_slurmdbd_mysql_db }}.*:ALL'
              state: present

          - name: install slurm accounting daemon from OpenHPC (slurm-slurmdbd-ohpc rpm)
            yum:
              name: slurm-slurmdbd-ohpc
              state: installed

          - name: deploy slurm accounting config file
            template:
              dest: /etc/slurm/slurmdbd.conf
              src: slurmdbd.conf.j2
              owner: root
              group: root
              mode: 0644
            notify: Restart slurmdbd service
            tags: slurm_config

          - name: Enable and start slurmdbd
            service:
              name: slurmdbd
              state: started
              enabled: true
            register: _enable_start_slurmdbd

          - name: Wait 15secs to let slurmdbd boot if this is the first time it boots
            pause:
              seconds: 15
            when: _enable_start_slurmdbd.changed

          - name: Check if the cluster is registered in accounting db
            shell: sacctmgr list cluster --noheader --parsable | wc -l
            register: _sacctmgr_output
            changed_when: false

          # - debug:
          #     var: _sacctmgr_output

          # if previous command return 0 lines it's because no cluster is registered
          # We assume that no other clusters are re
          - name: Register the cluster in accounting db
            command: sacctmgr add cluster slurm_cluster --immediate
            when: _sacctmgr_output.stdout == "0"

      when: "'slurm_master_group' in group_names"


    - name: Install and configure slurm master daemon
      block:

        - name: Install slurm master rpms
          yum:
            name:
              - "@ohpc-slurm-server"
              - "slurm-slurmctld-ohpc"
              - "slurm-example-configs-ohpc"
            state: installed

        - name: Deploy /etc/slurm/job_submit.lua
          template:
            dest: /etc/slurm/job_submit.lua
            src: job_submit.lua.j2
            owner: root
            group: root
            mode: 0755
          tags: slurm_config

        - name: Start and enable slurm master daemon
          service:
            name: slurmctld
            state: started
            enabled: true

      when: "'slurm_master_group' in group_names"


    - name: Configure slurm worker daemons
      block:

          - name: Install OpenHPC slurmd
            yum:
              name:
                - "slurm-slurmd-ohpc"
              state: present

          - name: Install cgroups if this is a worker host
            yum:
              name:
                - libcgroup
                - libcgroup-tools
              state: present

          - name: deploy /etc/slurm/cgroup.conf
            template:
              dest: /etc/slurm/cgroup.conf
              src: cgroup.conf.j2
              owner: root
              group: root
              mode: 0644
            notify: Restart SLURM service
            tags: slurm_config

          - name: enable and start cgroup services if this is a worker_node
            service:
              name: "{{ item }}"
              enabled: true
              state: started
            with_items:
              - cgred
              - cgconfig

          - name: Install slurm worker rpms
            yum:
              name:
                - "@ohpc-base-compute"
                - "@ohpc-slurm-client"
                - "lmod-ohpc"
              state: installed

          - name: Start and enable slurm worker daemon
            service:
              name: slurmd
              state: started
              enabled: true

      when: "'slurm_compute_group' in group_names"


    - name: Install software stack from compute canada
      block:

        - name: Query the size of the root partition in MB
          shell: df -m / | awk {'print $2'} |tail -1
          register: _root_partition_disk_size
          changed_when: false

        - name: Set a variable in MB with the disk size we will use for CVMFS cache (30% of root partition)
          set_fact:
            cvmfs_cache_size: "{{ ( _root_partition_disk_size.stdout|int * 0.3) | int }}"
            #cvmfs_cache_size: "{{ ( _root_partition_disk_size.stdout|int * 0.3) | round }}"

        - debug:
            var: cvmfs_cache_size
            verbosity: 2

        - name: Configure the CVMFS client to access the ComputeCanada soft stack
          import_role:
            name: ansible-cvmfs-client
          vars:
            cvmfs_http_proxy: "DIRECT"

        - name: Create profile file to enable the ComputeCanada software stack on login
          blockinfile:
            dest: /etc/profile.d/compute_canada.sh
            create: yes
            owner: root
            group: root
            mode: 0644
            marker: "# {mark} CREATED WITH ANSIBLE"
            content: |
              # we create all students accounts with uid>2000
              # and we only enable the software stack by default for them
              if [ "$UID" -ge 2000 ]; then
                  if [ -f /cvmfs/soft.computecanada.ca/config/profile/bash.sh ]; then
                      source /cvmfs/soft.computecanada.ca/config/profile/bash.sh &> /dev/null
                      #module load gcc/9.1.0 python/3.8.2 &> /dev/null
                  fi
              fi

      when: (slurm_cluster_enable_compute_canada_software_stack) and
            (('slurm_compute_group' in group_names) or ('slurm_login_group' in group_names))
      tags: soft


  handlers:

    - name: Restart Munge service
      service:
        name: munge
        state: restarted

    - name: Restart slurmctld service
      listen: Restart SLURM service
      service:
        name: slurmctld
        state: restarted
      when: "'slurm_master_group' in group_names"

    - name: Restart slurmd service
      listen: Restart SLURM service
      service:
        name: slurmd
        state: restarted
      when: "'slurm_compute_group' in group_names"

    - name: Restart slurmdbd service
      service:
        name: slurmdbd
        state: restarted
