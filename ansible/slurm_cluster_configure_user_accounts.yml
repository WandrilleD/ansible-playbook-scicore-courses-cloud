---

- name: Configure the user accounts
  hosts:
    - slurm_nfs_server_group
    - slurm_master_group
    - slurm_compute_group
    - slurm_login_group
  gather_facts: true
  become: true
  remote_user: "{{ slurm_cluster_ssh_remote_user }}"

  tasks:

    - name: Create a default group "slurm_cluster" for every user
      group:
        name: "slurm_cluster"
        gid: 2000
        state: present

    # We do this in the NFS server to make sure that root can access the home folders (just in case root squash is enabled)
    - name: Create the user accounts (starting with uid 2001) | First in the NFS server to create home folders
      user:
        name: "user{{ item }}"
        uid: "20{{ item }}"
        group: "slurm_cluster"
        home: "{{ slurm_cluster_users_home_folder }}/user{{ item }}"
        create_home: yes
        shell: /bin/bash
        update_password: always
        password: "{{ slurm_cluster_users_default_password }}"
        generate_ssh_key: yes
        ssh_key_comment: "user{{item}}_slurm_cluster@ansible-generated"
        state: present
        ssh_key_file: .ssh/id_rsa
        ssh_key_type: rsa
      with_sequence: "count={{ slurm_cluster_users_accounts }} format=%02u"
      when: "'slurm_nfs_server_group' in group_names"

    - name: Create the user accounts (starting with uid 2001) | Now in every machine
      user:
        name: "user{{ item }}"
        uid: "20{{ item }}"
        group: "slurm_cluster"
        home: "{{ slurm_cluster_users_home_folder }}/user{{ item }}"
        create_home: yes
        shell: /bin/bash
        update_password: always
        password: "{{ slurm_cluster_users_default_password }}"
        generate_ssh_key: yes
        ssh_key_comment: "user{{item}}_slurm_cluster@ansible-generated"
        state: present
        ssh_key_file: .ssh/id_rsa
        ssh_key_type: rsa
      with_sequence: "count={{ slurm_cluster_users_accounts }} format=%02u"

    # We do this in the NFS server to make sure that root can access the home folders (just in case root squash is enabled)
    - name: Get the ssh public keys for every user
      slurp:
        src: "{{ slurm_cluster_users_home_folder }}/user{{ item }}/.ssh/id_rsa.pub"
      register: _all_users_ssh_public_keys
      with_sequence: "count={{ slurm_cluster_users_accounts }} format=%02u"
      when: "'slurm_nfs_server_group' in group_names"

    # We do this in the NFS server to make sure that root can access the home folders (just in case root squash is enabled)
    - name: Add the user ssh keys to their accounts
      authorized_key:
        user: "user{{ item.item }}"
        key: "{{ item.content | b64decode }}"
      loop: "{{ _all_users_ssh_public_keys.results }}"
      when: "'slurm_nfs_server_group' in group_names"

    # We do this in the NFS server to make sure that root can access the home folders (just in case root squash is enabled)
    - name: Deploy custom ssh config for each user
      blockinfile:
        path: "{{ slurm_cluster_users_home_folder }}/user{{ item }}/.ssh/config"
        create: yes
        backup: yes
        owner: "user{{ item }}"
        mode: 0600
        marker: "# {mark} ANSIBLE MANAGED SSH CONFIG"
        block: |
          Host *
              StrictHostKeyChecking no
              ServerAliveInterval 10
      with_sequence: "count={{ slurm_cluster_users_accounts }} format=%02u"
      when: "'slurm_nfs_server_group' in group_names"

    - name: Check how many users are already registered in the Slurm DB
      shell: sacctmgr show users --noheader --parsable | wc -l
      register: _sacctmgr_users_list_output
      changed_when: false
      when: "'slurm_login_group' in group_names"

    # Only execute this task if previous command returns less lines than the number of students in the course
    - name: Register all the users in the Slurm DB
      command: "sacctmgr create user name=user{{ item }} DefaultAccount=root --immediate"
      with_sequence: "count={{ slurm_cluster_users_accounts }} format=%02u"
      when:
        - "'slurm_login_group' in group_names"
        - "_sacctmgr_users_list_output.stdout|int <= slurm_cluster_users_accounts|int"

    - name: Configure a cgroups memory limit in login node
      import_role:
        name: pescobar.cgroups_mem_limit
      vars:
        cgroup_to_whom_the_limit_applies: '@slurm_cluster'
      when: "'slurm_login_group' in group_names"
