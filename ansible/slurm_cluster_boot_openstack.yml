---

- name: Boot all the resources for the Slurm cluster in an OpenStack cloud
  hosts: localhost

  tasks:

    - name: Create a ssh keypair
      openssh_keypair:
        path: "{{ slurm_cluster_ssh_key_path }}"

    - name: Register ssh key in the cloud
      os_keypair:
        name: "{{ slurm_cluster_ssh_key_name }}"
        public_key_file: "{{ slurm_cluster_ssh_key_path }}.pub"

    - name: Query os project subnet info (filtering by project) | Default
      os_subnets_info:
        filters:
          project_id: "{{ ansible_env.OS_PROJECT_ID }}"
      register: _output

    # this task is only executed if openstack_subnets[] is NOT an empty list
    - name: Set var _os_project_net_info with the os project IPV4 net details | Default
      set_fact:
        _os_project_subnet_info: "{{ item }}"
      loop: "{{ _output.openstack_subnets }}"
      when:
        - _output.openstack_subnets | length > 0
        - item.ip_version|int == 4

    # if filtering by project didn't work we try without any filter
    - name: Query os project net info (without filtering by project)
      block:

        - name: Query os project net info (without any filter)
          os_subnets_info:
          register: _output

        - name: Set var _os_net_info with the os project IPV4 net details
          set_fact:
            _os_project_subnet_info: "{{ item }}"
          loop: "{{ _output.openstack_subnets }}"
          when: item.ip_version|int == 4

      when: _os_project_subnet_info is not defined

    - name: Set ansible variable with the os project internal network name
      set_fact:
        _os_project_network_name: "{{ _os_project_subnet_info.name }}"

    - name: Set ansible variable with the os project internal network id
      set_fact:
        _os_project_network_id: "{{ _os_project_subnet_info.network_id }}"

    - name: Set ansible variable with the os project internal subnet cidr
      set_fact:
        _os_project_subnet_cidr: "{{ _os_project_subnet_info.cidr }}"

    - name: Create default security group for the slurm cluster
      os_security_group:
        name: "{{ slurm_cluster_security_group_default }}"
        description: "Security group for the slurm cluster allowing any internal traffic"

    - name: Add default fw rules to allow any internal traffic in the slurm cluster
      os_security_group_rule:
        security_group: "{{ slurm_cluster_security_group_default }}"
        direction: ingress
        remote_ip_prefix: "{{ _os_project_subnet_cidr }}"

    - name: Create security group for the login node
      os_security_group:
        name: "{{ slurm_cluster_security_group_login }}"
        description: "Security group for the login node in the slurm cluster"

    - name: Add fw rules for the login node (allowed access from the internet)
      os_security_group_rule:
        security_group: "{{ slurm_cluster_security_group_login }}"
        direction: ingress
        protocol: tcp
        port_range_min: "{{ item }}"
        port_range_max: "{{ item }}"
        remote_ip_prefix: 0.0.0.0/0
      loop: "{{ slurm_cluster_open_ports_in_login_node }}"

    - name: Launch the login node and attach a floating ip to it
      os_server:
        name: "slurm-login"
        image: "{{ slurm_cluster_login_image }}"
        key_name: "{{ slurm_cluster_ssh_key_name }}"
        flavor: "{{ slurm_cluster_login_flavor }}"
        network: "{{ _os_project_network_id }}"
        security_groups:
          - "{{ slurm_cluster_security_group_default }}"
          - "{{ slurm_cluster_security_group_login }}"
        floating_ip_pools: "{{ slurm_cluster_floating_ips_pool | default('public') }}"
        meta:
          hostname: "slurm-login"
      register: _slurm_login_info


    - name: Generate the required ssh config to use login node as bastion host
      block:

        - name: Define a list of dicts with all the ips in the login node
          set_fact:
            _slurm_login_list_of_ips: "{{ item.value }}"
          with_dict: "{{ _slurm_login_info.openstack.addresses }}"

        # We assume _slurm_login_list_of_ips[0] is always the project/tenant private ip
        - name: Set variable with private (tenant) ip of login node
          set_fact:
            _slurm_login_internal_ip: "{{  _slurm_login_list_of_ips[0].addr | ipv4 }}"

        # We assume that last element in _slurm_login_list_of_ips[] is the public/floating ip
        # sciCORE cloud only provide 2 ips by VM in _slurm_login_list_of_ips[]
        # Switch engines cloud uses ipv6 so _slurm_login_internal_ip[] has 3 elements
        # _slurm_login_list_of_ips[0] > internal private ip
        # _slurm_login_list_of_ips[1] > ipv6 address
        # _slurm_login_list_of_ips[2] > floating ip
        - name: Set variable with public (floating) IPs of the login node
          set_fact:
            _slurm_login_floating_ip: "{{ _slurm_login_list_of_ips[-1].addr | ipv4 }}"

        # this regex replaces the internal ip like 192.168.22.33 to 192.168.*.*
        - name: Set variable with the os project internal network range as required by ssh config e.g. 192.168.*.*
          set_fact:
            _os_project_internal_ip_range: "{{ _slurm_login_internal_ip | regex_replace('([0-9]+).([0-9]+)$','*.*') }}"
          when: _os_project_internal_ip_range is not defined

        - name: Create a ssh config to connect to any machine in the slurm cluster using login node as bastion host
          template:
            src: ssh_config.j2
            dest: "{{ slurm_cluster_ssh_cfg_path }}"
            mode: 0600


    - name: Add login machine to in-memory ansible inventory. Choose the right groups depending on the services hosted in the login node
      block:

        - name: Add login machine to in-memory ansible inventory. NFS and slurm master run in login node
          add_host:
            name: slurm-login
            groups:
              - slurm_cluster_all
              - slurm_login_group
              - slurm_nfs_server_group
              - slurm_master_group
            ansible_host: "{{ _slurm_login_floating_ip }}"
            ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
            ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
            ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
          changed_when: false
          when:
            - not slurm_cluster_nfs_server_dedicated_machine
            - not slurm_cluster_slurm_master_dedicated_machine

        - name: Add login machine to in-memory ansible inventory. NFS runs in login node. Slurm master not
          add_host:
            name: slurm-login
            groups:
              - slurm_cluster_all
              - slurm_login_group
              - slurm_nfs_server_group
            ansible_host: "{{ _slurm_login_floating_ip }}"
            ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
            ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
            ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
          changed_when: false
          when:
            - not slurm_cluster_nfs_server_dedicated_machine
            - slurm_cluster_slurm_master_dedicated_machine

        - name: Add login machine to in-memory ansible inventory. Slurm master runs in login. NFS not
          add_host:
            name: slurm-login
            groups:
              - slurm_cluster_all
              - slurm_login_group
              - slurm_master_group
              - slurm_nfs_clients_group
            ansible_host: "{{ _slurm_login_floating_ip }}"
            ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
            ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
            ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
          changed_when: false
          when:
            - slurm_cluster_nfs_server_dedicated_machine
            - not slurm_cluster_slurm_master_dedicated_machine

        - name: Add login machine to in-memory ansible inventory. Neither NFS or slurm master run in login
          add_host:
            name: slurm-login
            groups:
              - slurm_cluster_all
              - slurm_login_group
              - slurm_nfs_clients_group
            ansible_host: "{{ _slurm_login_floating_ip }}"
            ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
            ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
            ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
          changed_when: false
          when:
            - slurm_cluster_nfs_server_dedicated_machine
            - slurm_cluster_slurm_master_dedicated_machine

    - name: Make sure we can ssh to the login node
      delegate_to: slurm-login
      wait_for_connection:

    - debug:
        msg: "To connect to to login node try: ssh -F {{ slurm_cluster_ssh_cfg_path }} slurm-login"

    - name: Launch the slurm compute nodes (CAN TAKE SOME TIME!)
      os_server:
        name: "slurm-compute-{{ item }}"
        image: "{{ slurm_cluster_compute_image }}"
        key_name: "{{ slurm_cluster_ssh_key_name }}"
        flavor: "{{ slurm_cluster_compute_flavor }}"
        security_groups:
          - "{{ slurm_cluster_security_group_default }}"
        network: "{{ _os_project_network_id }}"
        auto_ip: no  # we don't need a public ip in the compute nodes. We will use the login node as ssh bastion host
        wait: no  # we don't wait until instance is active so we can boot more servers in parallel
        meta:
          hostname: "slurm-compute-{{ item }}"
      with_sequence: count={{ slurm_cluster_num_workers }} format=%02u

    # first element of the list is the last booted server so once first one is ACTIVE we
    # assume that the other compute nodes which were booted before are also active
    - name: Waiting for the slurm compute nodes to be spawned so we get the ip info (status = ACTIVE)
      os_server_info:
        server: "slurm-compute*"
      register: _os_slurm_compute_nodes_info
      until: "_os_slurm_compute_nodes_info.openstack_servers[0].status == 'ACTIVE'"
      retries: 40
      delay: 15

    # Switch Engines cloud returns the VM's private ip in field "private_v4" and returns empty value in "accessIPv4"
    # sciCORE cloud returns the VM's private ip in field "accessIPv4" and empty value in "private_v4"
    # That's why we use an "if" to choose the right value for argument "ansible_host" in this task
    - name: Add compute nodes to in-memory ansible inventory (Switch Engines provides private ip in field private_v4)
      add_host:
        name: "{{ item.name }}"
        groups:
          - slurm_cluster_all
          - slurm_compute_group
          - slurm_nfs_clients_group
        ansible_host: "{% if item.private_v4 | ipaddr %}{{ item.private_v4 }}{% else %}{{ item.accessIPv4 | ipaddr }}{% endif %}"
        ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
        ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
        ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
      loop: "{{ _os_slurm_compute_nodes_info.openstack_servers }}"
      changed_when: false

    - name: Make sure we can ssh to the compute nodes
      delegate_to: "{{ item.name }}"
      wait_for_connection:
      loop: "{{ _os_slurm_compute_nodes_info.openstack_servers }}"

    - name: Launch dedicated machine for NFS server if slurm_cluster_nfs_server_dedicated_machine=true
      block:

        - name: Launch the NFS server machine
          os_server:
            name: "slurm-nfs-server"
            image: "{{ slurm_cluster_nfs_server_image }}"
            key_name: "{{ slurm_cluster_ssh_key_name }}"
            flavor: "{{ slurm_cluster_nfs_server_flavor }}"
            security_groups:
              - "{{ slurm_cluster_security_group_default }}"
            network: "{{ _os_project_network_id }}"
            auto_ip: no  # we don't need a public ip
            wait: yes
            meta:
              hostname: "slurm-nfs-server"
          register: _slurm_nfs_server_info

        - name: Add slurm-nfs-server to in-memory ansible inventory
          add_host:
            name: "{{ _slurm_nfs_server_info.openstack.name }}"
            groups:
              - slurm_cluster_all
              - slurm_nfs_server_group
            ansible_host: "{% if _slurm_nfs_server_info.openstack.private_v4 | ipaddr %}{{ _slurm_nfs_server_info.openstack.private_v4 }}\
                           {% else %}\
                           {{ _slurm_nfs_server_info.openstack.accessIPv4 | ipaddr }}{% endif %}"
            ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
            ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
            ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
          changed_when: false

        - name: Waiting for the slurm nfs server to be online
          delegate_to: "{{ _slurm_nfs_server_info.openstack.name }}"
          wait_for_connection:

      when: slurm_cluster_nfs_server_dedicated_machine | bool

    - name: Launch dedicated machine for slurm master if slurm_cluster_slurm_master_dedicated_machine=True
      block:

        - name: Launch the slurm master machine
          os_server:
            name: "slurm-master"
            image: "{{ slurm_cluster_slurm_master_image }}"
            key_name: "{{ slurm_cluster_ssh_key_name }}"
            flavor: "{{ slurm_cluster_slurm_master_flavor }}"
            security_groups:
              - "{{ slurm_cluster_security_group_default }}"
            network: "{{ _os_project_network_id }}"
            auto_ip: no  # we don't need a public ip
            wait: yes
            meta:
              hostname: "slurm-master"
          register: _slurm_master_info

        - name: Add slurm master to in-memory ansible inventory
          add_host:
            name: "{{ _slurm_master_info.openstack.name }}"
            groups:
              - slurm_cluster_all
              - slurm_master_group
            ansible_host: "{% if _slurm_master_info.openstack.private_v4 | ipaddr %}{{ _slurm_master_info.openstack.private_v4 }}\
                           {% else %}\
                           {{ _slurm_master_info.openstack.accessIPv4 | ipaddr }}{% endif %}"
            ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
            ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
            ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
          changed_when: false

        - name: Waiting for the slurm master to be online
          delegate_to: "{{ _slurm_master_info.openstack.name }}"
          wait_for_connection:

      when: slurm_cluster_slurm_master_dedicated_machine | bool

    - name: Create a cinder volume to export by nfs
      os_volume:
        display_name: "slurm_cluster_nfs_data"
        size: "{{ slurm_cluster_nfs_server_disk_size }}"
        volume_type: "{{ slurm_cluster_nfs_server_disk_volume_type | default(omit) }}"
        state: present

    - name: Attach NFS data volume to NFS server
      block:

        - name: Attach NFS volume to login (if NFS server runs in login)
          os_server_volume:
            server: "slurm-login"
            volume: "slurm_cluster_nfs_data"
            device: "{{ slurm_cluster_nfs_server_disk_device }}"
          when: not slurm_cluster_nfs_server_dedicated_machine

        - name: Attach NFS volume to dedicated NFS server
          os_server_volume:
            server: "slurm-nfs-server"
            volume: "slurm_cluster_nfs_data"
            device: "{{ slurm_cluster_nfs_server_disk_device }}"
          when: slurm_cluster_nfs_server_dedicated_machine


    # Left here as reference
    #
    # An alternative to build the static inventory is to use ansible module "os_server_info"
    # but this would require a specific template adapted to the output of "os_server_info"
    #
    # With the current approach template "ansible_inventory.j2" uses "hostvars" which should be
    # easier to port to a different cloud in case we need it in the future
    #
    # - name: Query information about all the cluster machines (those using the cluster ssh key)
    #   os_server_info:
    #     filters:
    #       key_name: "{{ slurm_cluster_ssh_key_name }}"
    #   environment:
    #     OS_PROJECT_DOMAIN_ID: "{{ lookup('env','OS_PROJECT_DOMAIN_ID') }}"
    #     OS_PROJECT_ID: "{{ lookup('env','OS_PROJECT_ID') }}"
    #     OS_REGION_NAME: "{{ lookup('env','OS_REGION_NAME') }}"
    #     OS_USER_DOMAIN_NAME: "{{ lookup('env','OS_USER_DOMAIN_NAME') }}"
    #     OS_PROJECT_NAME: "{{ lookup('env','OS_PROJECT_NAME') }}"
    #     OS_IDENTITY_API_VERSION: "{{ lookup('env','OS_IDENTITY_API_VERSION') }}"
    #     OS_PASSWORD: "{{ lookup('env','OS_PASSWORD') }}"
    #     OS_AUTH_URL: "{{ lookup('env','OS_AUTH_URL') }}"
    #     OS_USERNAME: "{{ lookup('env','OS_USERNAME') }}"
    #     OS_INTERFACE: "{{ lookup('env','OS_INTERFACE') }}"
    #   register: _os_server_info

    # - name: Create the ansible inventory
    #   template:
    #     dest: inventory/hosts
    #     src: ansible_inventory.j2


# Now that we have booted all the resources we run another playbook.
# It's important that this play uses "hosts:all" so we run the facter
# in every machine we have booted above (which are in the in-memory inventory).
# Once we collect facts for all the machines we create an static ansible inventory
# in "ansible/inventory/hosts" using the template in "ansible/templates/ansible_inventory.j2"
# It's very important in which groups are the hosts in the in-memory inventory. Check
# the template for details

- name: Create the ansible inventory
  hosts: all
  become: yes
  gather_facts: yes
  remote_user: "{{ slurm_cluster_ssh_remote_user }}"

  tasks:

    - name: Create the inventory folder
      become: no
      delegate_to: localhost
      file:
        path: "{{ playbook_dir }}/inventory/"
        state: directory
        mode: 0755

    - name: Create the ansible static inventory
      become: no
      delegate_to: localhost
      template:
        dest: inventory/hosts
        src: ansible_inventory.j2
        mode: 0644
