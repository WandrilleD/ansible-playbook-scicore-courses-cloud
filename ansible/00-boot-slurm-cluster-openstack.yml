---

- name: Create security groups and ssh key
  hosts: localhost

  tasks:

    - name: Check if we are using Switch Engines cloud to apply required exceptions
      set_fact:
        _switch_engines_cloud: "{% if ansible_env.OS_AUTH_URL | regex_search('(cloud.switch.ch)') %}True{% else %}False{% endif %}"

    - name: Create a ssh keypair for the slurm cluster
      openssh_keypair:
        path: "{{ slurm_cluster_ssh_key_path }}"

    - name: Register ssh key in openstack
      os_keypair:
        name: "{{ slurm_cluster_ssh_key_name }}"
        public_key_file: "{{ slurm_cluster_ssh_key_path }}.pub"

    - name: Query subnet info (SwitchEngines)
      block:

          # Switch engines cloud doesn't allow to filter by project.
          # We have to use "os_subnets_info" without any argument and we get the subnet info for current project
          - name: Query subnet info for the project (SwitchEngines)
            os_subnets_info:
            register: _subnet_info_switch_cloud

          - name:
            set_fact:
              _subnet_info: "{{ _subnet_info_switch_cloud }}"

      when: _switch_engines_cloud | bool


    - name: Query subnet info (filtering by project)
      block:

          - name: Query subnet info (filtering by project)
            os_subnets_info:
              filters:
                project_id: "{{ ansible_env.OS_PROJECT_ID }}"
            register: _subnet_info_others

          - name:
            set_fact:
              _subnet_info: "{{ _subnet_info_others }}"

      when: _subnet_info is not defined


    - name: Set ansible variable with the os project internal network name
      set_fact:
        _os_project_network_name: "{{ _subnet_info.openstack_subnets[0].name }}"

    - name: Set ansible variable with the os project internal network id
      set_fact:
        _os_project_network_id: "{{ _subnet_info.openstack_subnets[0].network_id }}"

    - name: Set ansible variable with the os project internal subnet cidr
      set_fact:
        _os_project_subnet_cidr: "{{ _subnet_info.openstack_subnets[0].cidr }}"

    - name: Create default security group for the slurm cluster
      os_security_group:
        name: "{{ slurm_cluster_security_group_default }}"
        description: "Security group for the slurm cluster allowing any internal traffic"

    - name: Add default fw rules to allow any internal traffic in the slurm cluster
      os_security_group_rule:
        security_group: "{{ slurm_cluster_security_group_default }}"
        direction: ingress
        remote_ip_prefix: "{{ _os_project_subnet_cidr }}"

    - name: Create security group for the login node
      os_security_group:
        name: "{{ slurm_cluster_security_group_login }}"
        description: "Security group for the login node in the slurm cluster"

    - name: Add fw rules for the login node (allowed access from the internet)
      os_security_group_rule:
        security_group: "{{ slurm_cluster_security_group_login }}"
        direction: ingress
        protocol: tcp
        port_range_min: "{{ item }}"
        port_range_max: "{{ item }}"
        remote_ip_prefix: 0.0.0.0/0
      loop: "{{ slurm_cluster_open_ports_in_login_node }}"

    - name: Launch the login node and attach a floating ip to it
      os_server:
        name: "slurm_login"
        image: "{{ slurm_cluster_login_image }}"
        key_name: "{{ slurm_cluster_ssh_key_name }}"
        flavor: "{{ slurm_cluster_login_flavor }}"
        network: "{{ _os_project_network_id }}"
        security_groups:
          - "{{ slurm_cluster_security_group_default }}"
          - "{{ slurm_cluster_security_group_login }}"
        floating_ip_pools: "{{ slurm_cluster_floating_ips_pool | default('public') }}"
        meta:
          hostname: "slurm_login"
          role: "slurm_login"
      register: _slurm_login_info


    - name: Generate the required ssh config to use login node as bastion host
      block:

          - name: Define a list of dicts with all the ips in the login node
            set_fact:
              _slurm_login_list_of_ips: "{{ item.value }}"
            with_dict: "{{ _slurm_login_info.openstack.addresses }}"

          # _slurm_login_list_of_ips[0] is always the project/tenant private ip
          - name: Set variable with private (tenant) ip of login node
            set_fact:
              _slurm_login_internal_ip: "{{  _slurm_login_list_of_ips[0].addr | ipv4 }}"

          # Switch engines uses ipv6 so _slurm_login_internal_ip has 3 elements
          # _slurm_login_list_of_ips[0] > internal private ip
          # _slurm_login_list_of_ips[1] > ipv6 address
          # _slurm_login_list_of_ips[2] > floating ip
          - name: Set variable with public (floating) IPs of the login node (Switch Engines)
            set_fact:
              _slurm_login_floating_ip: "{{ _slurm_login_list_of_ips[2].addr | ipv4 }}"
            when: _switch_engines_cloud

          # by default we assume _slurm_login_list_of_ips[1] is the public floating ip
          - name: Set variables with public (floating) IPs of the login node (Default)
            set_fact:
              _slurm_login_floating_ip: "{{ _slurm_login_list_of_ips[1].addr | ipv4 }}"
            when: _slurm_login_floating_ip is not defined

          # this regex replaces the internal ip like 192.168.22.33 to 192.168.*.*
          - name: Set variable with the os project internal network range as required by ssh config e.g. 192.168.*.*
            set_fact:
              _os_project_internal_ip_range: "{{ _slurm_login_internal_ip | regex_replace('([0-9]+).([0-9]+)$','*.*') }}"
            when: _os_project_internal_ip_range is not defined

          - name: Create a ssh config to connect to any machine in the slurm cluster using login node as bastion host
            template:
              src: ssh_config.j2
              dest: "{{ slurm_cluster_ssh_cfg_path }}"


    - name: Add login machine to in-memory ansible inventory. Choose the right groups depending on the services hosted in the login node
      block:

          - name: Add login machine to in-memory ansible inventory. NFS and slurm master run in login node
            add_host:
              name: slurm_login
              groups:
                - slurm_cluster_all
                - slurm_login_group
                - slurm_nfs_server_group
                - slurm_master_group
              ansible_host: "{{ _slurm_login_floating_ip }}"
              ansible_default_ipv4: "{{ _slurm_login_internal_ip }}"
              ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
              ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
              ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
            changed_when: false
            when:
              - not slurm_cluster_nfs_server_dedicated_machine
              - not slurm_cluster_slurm_master_dedicated_machine

          - name: Add login machine to in-memory ansible inventory. NFS runs in login node. Slurm master not
            add_host:
              name: slurm_login
              groups:
                - slurm_cluster_all
                - slurm_login_group
                - slurm_nfs_server_group
              ansible_host: "{{ _slurm_login_floating_ip }}"
              ansible_default_ipv4: "{{ _slurm_login_internal_ip }}"
              ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
              ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
              ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
            changed_when: false
            when:
              - not slurm_cluster_nfs_server_dedicated_machine
              - slurm_cluster_slurm_master_dedicated_machine

          - name: Add login machine to in-memory ansible inventory. Slurm master runs in login. NFS not
            add_host:
              name: slurm_login
              groups:
                - slurm_cluster_all
                - slurm_login_group
                - slurm_master_group
                - slurm_nfs_clients_group
              ansible_host: "{{ _slurm_login_floating_ip }}"
              ansible_default_ipv4: "{{ _slurm_login_internal_ip }}"
              ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
              ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
              ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
            changed_when: false
            when:
              - slurm_cluster_nfs_server_dedicated_machine
              - not slurm_cluster_slurm_master_dedicated_machine

          - name: Add login machine to in-memory ansible inventory. Neither NFS or slurm master run in login
            add_host:
              name: slurm_login
              groups:
                - slurm_cluster_all
                - slurm_login_group
                - slurm_nfs_clients_group
              ansible_host: "{{ _slurm_login_floating_ip }}"
              ansible_default_ipv4: "{{ _slurm_login_internal_ip }}"
              ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
              ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
              ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
            changed_when: false
            when:
              - slurm_cluster_nfs_server_dedicated_machine
              - slurm_cluster_slurm_master_dedicated_machine

    - name: Waiting for the login machine to be online
      delegate_to: slurm_login
      wait_for_connection:

    - debug:
        msg: "To connecto to login node try: ssh -F {{ slurm_cluster_ssh_cfg_path }} slurm_login"

    # We need to wait for each machine to boot so we get the IP info. This is done secuencially. One machine at a time
    # This means that this task can take some time depending on the amount of compute nodes and their size
    - name: Launch the slurm compute nodes (CAN TAKE SOME TIME!)
      os_server:
        name: "slurm_compute_{{ item }}"
        image: "{{ slurm_cluster_compute_image }}"
        key_name: "{{ slurm_cluster_ssh_key_name }}"
        flavor: "{{ slurm_cluster_compute_flavor }}"
        security_groups:
          - "{{ slurm_cluster_security_group_default }}"
        network: "{{ _os_project_network_id }}"
        auto_ip: no
        meta:
          hostname: "slurm_compute_{{ item }}"
          role: "slurm_compute"
      with_sequence: count={{ slurm_cluster_num_workers }} format=%02u
      register: _slurm_compute_nodes_info

    # Switch Engines cloud returns the VM's private ip in field "private_v4" but returns empty value in "accessIPv4"
    # sciCORE cloud returns the VM's private ip in field "accessIPv4" but empty value in "private_v4"
    # That's why we use an "if" to choose the right value for argument "ansible_host" in this task
    - name: Add compute nodes to in-memory ansible inventory (Switch Engines provides private ip in field private_v4)
      add_host:
        name: "{{ item.openstack.name }}"
        groups:
          - slurm_cluster_all
          - slurm_compute_group
          - slurm_nfs_clients_group
        ansible_host: "{% if item.openstack.private_v4 | ipaddr %}{{ item.openstack.private_v4 }}{% else %}{{ item.openstack.accessIPv4 | ipaddr }}{% endif %}"
        ansible_default_ipv4: "{% if item.openstack.private_v4 | ipaddr %}{{ item.openstack.private_v4 }}{% else %}{{ item.openstack.accessIPv4 | ipaddr }}{% endif %}"
        ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
        ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
        ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
      loop: "{{ _slurm_compute_nodes_info.results }}"
      changed_when: false

    - name: Waiting for the slurm compute nodes to be online
      delegate_to: "{{ item.openstack.name }}"
      wait_for_connection:
      loop: "{{ _slurm_compute_nodes_info.results }}"

    - name: Launch dedicated machine for NFS server if slurm_cluster_nfs_server_dedicated_machine=true
      block:

          - name: Launch the NFS server machine
            os_server:
              name: "slurm_nfs_server"
              image: "{{ slurm_cluster_nfs_server_image }}"
              key_name: "{{ slurm_cluster_ssh_key_name }}"
              flavor: "{{ slurm_cluster_nfs_server_flavor }}"
              security_groups:
                - "{{ slurm_cluster_security_group_default }}"
              network: "{{ _os_project_network_id }}"
              meta:
                hostname: "slurm_nfs_server"
                role: "slurm_nfs_server"
            register: _slurm_nfs_server_info

          - name: Add slurm_nfs_server to in-memory ansible inventory
            add_host:
              name: "{{ _slurm_nfs_server_info.openstack.name }}"
              groups:
                - slurm_cluster_all
                - slurm_nfs_server_group
              ansible_host: "{% if item.openstack.private_v4 | ipaddr %}{{ item.openstack.private_v4 }}{% else %}{{ item.openstack.accessIPv4 | ipaddr }}{% endif %}"
              ansible_default_ipv4: "{% if item.openstack.private_v4 | ipaddr %}{{ item.openstack.private_v4 }}{% else %}{{ item.openstack.accessIPv4 | ipaddr }}{% endif %}"
              ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
              ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
              ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
            changed_when: false

          - name: Waiting for the slurm nfs server to be online
            delegate_to: "{{ _slurm_nfs_server_info.openstack.name }}"
            wait_for_connection:

      when: slurm_cluster_nfs_server_dedicated_machine | bool

    - name: Launch dedicated machine for slurm master if slurm_cluster_slurm_master_dedicated_machine=True
      block:

          - name: Launch the slurm master machine
            os_server:
              name: "slurm_master"
              image: "{{ slurm_cluster_slurm_master_image }}"
              key_name: "{{ slurm_cluster_ssh_key_name }}"
              flavor: "{{ slurm_cluster_slurm_master_flavor }}"
              security_groups:
                - "{{ slurm_cluster_security_group_default }}"
              network: "{{ _os_project_network_id }}"
              meta:
                hostname: "slurm_master"
                role: "slurm_master"
            register: _slurm_master_info

          - name: Add slurm master to in-memory ansible inventory
            add_host:
              name: "{{ _slurm_master_info.openstack.name }}"
              groups:
                - slurm_cluster_all
                - slurm_master_group
              ansible_host: "{% if item.openstack.private_v4 | ipaddr %}{{ item.openstack.private_v4 }}{% else %}{{ item.openstack.accessIPv4 | ipaddr }}{% endif %}"
              ansible_default_ipv4: "{% if item.openstack.private_v4 | ipaddr %}{{ item.openstack.private_v4 }}{% else %}{{ item.openstack.accessIPv4 | ipaddr }}{% endif %}"
              ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
              ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
              ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
            changed_when: false

          - name: Waiting for the slurm master to be online
            delegate_to: "{{ _slurm_master_info.openstack.name }}"
            wait_for_connection:

      when: slurm_cluster_slurm_master_dedicated_machine | bool

    - name: Create a cinder volume to export by nfs
      os_volume:
        display_name: "slurm_cluster_nfs_data"
        size: "{{ slurm_cluster_nfs_server_disk_size }}"
        state: present

    - name: Attach NFS data volume to NFS server
      block:

          - name: Attach NFS volume to login (if NFS server runs in login)
            os_server_volume:
              server: "slurm_login"
              volume: "slurm_cluster_nfs_data"
              device: /dev/vdb
            when: not slurm_cluster_nfs_server_dedicated_machine

          - name: Attach NFS volume to dedicated NFS server
            os_server_volume:
              server: "slurm_nfs_server"
              volume: "slurm_cluster_nfs_data"
              device: "{{ slurm_cluster_nfs_server_disk_device }}"
            when: slurm_cluster_nfs_server_dedicated_machine


    # Left here as reference
    #
    # An alternative to build the static inventory is to use ansible module "os_server_info"
    # but this would require a specific template adapted to the output of "os_server_info"
    #
    # With the current approach template "ansible_inventory.j2" uses "hostvars" which should be
    # easier to port to a different cloud in case we need it in the future
    #
    # - name: Query information about all the cluster machines (those using the cluster ssh key)
    #   os_server_info:
    #     filters:
    #       key_name: "{{ slurm_cluster_ssh_key_name }}"
    #   environment:
    #     OS_PROJECT_DOMAIN_ID: "{{ lookup('env','OS_PROJECT_DOMAIN_ID') }}"
    #     OS_PROJECT_ID: "{{ lookup('env','OS_PROJECT_ID') }}"
    #     OS_REGION_NAME: "{{ lookup('env','OS_REGION_NAME') }}"
    #     OS_USER_DOMAIN_NAME: "{{ lookup('env','OS_USER_DOMAIN_NAME') }}"
    #     OS_PROJECT_NAME: "{{ lookup('env','OS_PROJECT_NAME') }}"
    #     OS_IDENTITY_API_VERSION: "{{ lookup('env','OS_IDENTITY_API_VERSION') }}"
    #     OS_PASSWORD: "{{ lookup('env','OS_PASSWORD') }}"
    #     OS_AUTH_URL: "{{ lookup('env','OS_AUTH_URL') }}"
    #     OS_USERNAME: "{{ lookup('env','OS_USERNAME') }}"
    #     OS_INTERFACE: "{{ lookup('env','OS_INTERFACE') }}"
    #   register: _os_server_info

    # - name: Create the ansible inventory
    #   template:
    #     dest: inventory/hosts
    #     src: ansible_inventory.j2


- name: Create the ansible inventory
  hosts: localhost
  become: yes
  gather_facts: yes
  remote_user: "{{ slurm_cluster_ssh_remote_user }}"

  tasks:

    - debug:
        var: hostvars

    - name: Create the ansible inventory
      become: no
      template:
        dest: inventory/hosts
        src: ansible_inventory.j2
