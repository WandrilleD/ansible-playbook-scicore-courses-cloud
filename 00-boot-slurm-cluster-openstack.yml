---

- name: Create security groups and ssh key
  hosts: localhost

  tasks:

    - name: Create a ssh keypair for the slurm cluster
      openssh_keypair:
        path: "{{ slurm_cluster_ssh_key_path }}"

    - name: Register ssh key in openstack
      os_keypair:
        name: "{{ slurm_cluster_ssh_key_name }}"
        public_key_file: "{{ slurm_cluster_ssh_key_path }}.pub"

    # ansible module os_subnets_info crashes whith Switch Engines cloud
    - name: Hardcode _os_project_subnet_cidr and _os_project_network_id for Switch cloud
      set_fact:
        _os_project_subnet_cidr: "10.0.0.0/16"
        _os_project_network_id: "private"
      when: "'{{ lookup('env', 'OS_AUTH_URL') }}' == 'https://keystone.cloud.switch.ch:5000/v3'"

    - name: Query network info from the OpenStack API if it's not hardcoded before
      block:

        - name: Query subnet info for the project
          os_subnets_info:
            filters:
              project_id: "{{ lookup('env', 'OS_PROJECT_ID') }}"
          register: _subnet_info

        - name: DEBUG | Print os project subnet info
          debug:
            var: _subnet_info
          when: slurm_cluster_debug | bool

        - name: Set ansible variable with the os project internal subnet cidr
          set_fact:
            _os_project_subnet_cidr: "{{ _subnet_info.openstack_subnets[0].cidr }}"
          when:

        - name: Set ansible variable with the os project internal network name
          set_fact:
            _os_project_network_name: "{{ _subnet_info.openstack_subnets[0].name }}"

        # this one must be last in the block or conditional will won't match
        - name: Set ansible variable with the os project internal network id
          set_fact:
            _os_project_network_id: "{{ _subnet_info.openstack_subnets[0].network_id }}"

      when: _os_project_network_id is not defined


    - name: Create default security group for the slurm cluster
      os_security_group:
        name: "{{ slurm_cluster_security_group_default }}"
        description: "Security group for the slurm cluster allowing any internal traffic"

    - name: Add default fw rules to allow any internal traffic in the slurm cluster
      os_security_group_rule:
        security_group: "{{ slurm_cluster_security_group_default }}"
        direction: ingress
        remote_ip_prefix: "{{ _os_project_subnet_cidr }}"

    - name: Create security group for the login node
      os_security_group:
        name: "{{ slurm_cluster_security_group_login }}"
        description: "Security group for the login node in the slurm cluster"

    - name: Add fw rules for the login node (allowed access from the internet)
      os_security_group_rule:
        security_group: "{{ slurm_cluster_security_group_login }}"
        direction: ingress
        protocol: tcp
        port_range_min: "{{ item }}"
        port_range_max: "{{ item }}"
        remote_ip_prefix: 0.0.0.0/0
      loop: "{{ slurm_cluster_open_ports_in_login_node }}"

    - name: Launch the login node and attach a floating ip to it
      os_server:
        name: "slurm_login"
        image: "{{ slurm_cluster_login_image }}"
        key_name: "{{ slurm_cluster_ssh_key_name }}"
        flavor: "{{ slurm_cluster_login_flavor }}"
        network: "{{ _os_project_network_id }}"
        security_groups:
          - "{{ slurm_cluster_security_group_default }}"
          - "{{ slurm_cluster_security_group_login }}"
        floating_ip_pools: "{{ slurm_cluster_floating_ips_pool | default('public') }}"
        meta:
          hostname: "slurm_login"
          role: "login"
          group: "nfs_clients"
      register: _slurm_login_info

    - name: DEBUG | Print login node info
      debug:
        var: _slurm_login_info
      when: slurm_cluster_debug

    - name: Generate the required ssh config to use login node as bastion host
      block:

        - name: Set variable with private (tenant) ip of login node
          set_fact:
            _slurm_login_internal_ip: "{{  _slurm_login_info.openstack.addresses.private[0].addr | ipv4('private') }}"

        - name: DEBUG | Print login node internal ip
          debug:
            var: _slurm_login_internal_ip
          when: slurm_cluster_debug

        - name: Set variable with public (floating) IPs of the login node (Switch Engines)
          set_fact:
            _slurm_login_floating_ip: "{{ _slurm_login_info.openstack.addresses.private[2].addr | ipv4('public') }}"
          when: "'{{ lookup('env', 'OS_AUTH_URL') }}' == 'https://keystone.cloud.switch.ch:5000/v3'"

        - name: Set variables with public (floating) IPs of the login node (Default)
          set_fact:
            _slurm_login_floating_ip: "{{ _slurm_login_info.openstack.addresses.private[1].addr | ipv4('public') }}"
          when: _slurm_login_floating_ip is not defined

        - name: DEBUG | Print public floating ip of login node
          debug:
            var: _slurm_login_floating_ip
          when: slurm_cluster_debug | bool

        - name: DEBUG | Print floating ip of login node
          debug:
            var: _slurm_login_floating_ip
          when: slurm_cluster_debug | bool

        - name: Set variable with the os project internal network range as required by ssh config e.g. 192.168.33.* (Switch Engines)
          set_fact:
            _os_project_internal_ip_range: "10.0.*.*"
          when: "'{{ lookup('env', 'OS_AUTH_URL') }}' == 'https://keystone.cloud.switch.ch:5000/v3'"

        - name: Set variable with the os project internal network range as required by ssh config e.g. 192.168.33.*
          set_fact:
            _os_project_internal_ip_range: "{{ _slurm_login_internal_ip | regex_replace('([0-9]+)$','*') }}"
          when: _os_project_internal_ip_range is not defined

        - name: DEBUG | Print _os_project_internal_ip_range to be used in the ssh config
          debug:
            var: _os_project_internal_ip_range
          when: slurm_cluster_debug | bool

        - name: Create a ssh config to connect to any machine in the slurm cluster
          template:
            src: ssh_config.j2
            dest: "{{ slurm_cluster_ssh_cfg_path }}"


    - name: Add login machine to in-memory ansible inventory. Choose the right groups depending on the services hosted in the login node
      block:

        - name: Add login machine to in-memory ansible inventory. NFS and slurm master run in login node
          add_host:
            name: slurm_login
            groups:
              - slurm_cluster_all
              - slurm_login_group
              - slurm_nfs_server_group
              - slurm_master_group
            ansible_host: "{{ _slurm_login_floating_ip }}"
            ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
            ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
            ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
          changed_when: false
          when:
            - not slurm_cluster_nfs_server_dedicated_machine
            - not slurm_cluster_slurm_master_dedicated_machine

        - name: Add login machine to in-memory ansible inventory. NFS runs in login node. Slurm master not
          add_host:
            name: slurm_login
            groups:
              - slurm_cluster_all
              - slurm_login_group
              - slurm_nfs_server_group
            ansible_host: "{{ _slurm_login_floating_ip }}"
            ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
            ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
            ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
          changed_when: false
          when:
            - not slurm_cluster_nfs_server_dedicated_machine
            - slurm_cluster_slurm_master_dedicated_machine

        - name: Add login machine to in-memory ansible inventory. Slurm master runs in login. NFS not
          add_host:
            name: slurm_login
            groups:
              - slurm_cluster_all
              - slurm_login_group
              - slurm_master_group
            ansible_host: "{{ _slurm_login_floating_ip }}"
            ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
            ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
            ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
          changed_when: false
          when:
            - slurm_cluster_nfs_server_dedicated_machine
            - not slurm_cluster_slurm_master_dedicated_machine

        - name: Add login machine to in-memory ansible inventory. Neither NFS or slurm master run in login
          add_host:
            name: slurm_login
            groups:
              - slurm_cluster_all
              - slurm_login_group
            ansible_host: "{{ _slurm_login_floating_ip }}"
            ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
            ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
            ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
          changed_when: false
          when:
            - slurm_cluster_nfs_server_dedicated_machine
            - slurm_cluster_slurm_master_dedicated_machine

    - name: Waiting for the login machine to be online
      delegate_to: slurm_login
      wait_for_connection:

    - debug:
        msg: "To connecto to login node try: ssh -F {{ slurm_cluster_ssh_cfg_path }} slurm_login"

    - name: Launch the slurm compute nodes
      os_server:
        name: "slurm_compute_{{ item }}"
        image: "{{ slurm_cluster_compute_image }}"
        key_name: "{{ slurm_cluster_ssh_key_name }}"
        flavor: "{{ slurm_cluster_compute_flavor }}"
        security_groups:
          - "{{ slurm_cluster_security_group_default }}"
        network: "{{ _os_project_network_id }}"
        wait: no
        meta:
          hostname: "slurm_compute_{{ item }}"
          role: "slurm_compute"
      with_sequence: count={{ slurm_cluster_num_workers }} format=%02u
      register: _slurm_compute_nodes_info

    - name: DEBUG | Print compute nodes info
      debug:
        var: _slurm_compute_nodes_info
      when: slurm_cluster_debug | bool

    - name: Add compute nodes to in-memory ansible inventory
      add_host:
        name: "{{ item.openstack.name }}"
        groups:
          - slurm_cluster_all
          - slurm_compute_group
        ansible_host: "{{ item.openstack.private_v4 }}"
        ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
        ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
        ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
      loop: "{{ _slurm_compute_nodes_info.results }}"
      changed_when: false

    - debug:
        var: _slurm_compute_nodes_info.results

    - name: Waiting for the slurm compute nodes to be online
      delegate_to: "{{ item.openstack.name }}"
      wait_for_connection:
      loop: "{{ _slurm_compute_nodes_info.results }}"

    - name: Launch dedicated machine for NFS server if slurm_cluster_nfs_server_dedicated_machine=true
      block:

        - name: Launch the NFS server machine
          os_server:
            name: "slurm_nfs_server"
            image: "{{ slurm_cluster_nfs_server_image }}"
            key_name: "{{ slurm_cluster_ssh_key_name }}"
            flavor: "{{ slurm_cluster_nfs_server_flavor }}"
            security_groups:
              - "{{ slurm_cluster_security_group_default }}"
            network: "{{ _os_project_network_id }}"
            wait: no
            meta:
              hostname: "slurm_nfs_server"
              role: "slurm_nfs_server"
          register: _slurm_nfs_server_info

        - name: DEBUG | Print nfs server info
          debug:
            var: _slurm_nfs_server_info
          when: slurm_cluster_debug | bool

        - name: Add slurm_nfs_server to in-memory ansible inventory
          add_host:
            name: "{{ _slurm_nfs_server_info.openstack.name }}"
            groups:
              - slurm_cluster_all
              - slurm_nfs_server_group
            ansible_host: "{{ _slurm_nfs_server_info.openstack.accessIPv4 }}"
            ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
            ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
            ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
          changed_when: false

        - name: Waiting for the slurm nfs server to be online
          delegate_to: "{{ _slurm_nfs_server_info.openstack.name }}"
          wait_for_connection:

      when: slurm_cluster_nfs_server_dedicated_machine | bool

    - name: Launch dedicated machine for slurm master if slurm_cluster_slurm_master_dedicated_machine=True
      block:

        - name: Launch the slurm master machine
          os_server:
            name: "slurm_master"
            image: "{{ slurm_cluster_slurm_master_image }}"
            key_name: "{{ slurm_cluster_ssh_key_name }}"
            flavor: "{{ slurm_cluster_slurm_master_flavor }}"
            security_groups:
              - "{{ slurm_cluster_security_group_default }}"
            network: "{{ _os_project_network_id }}"
            wait: no
            meta:
              hostname: "slurm_master"
              role: "slurm_master"
          register: _slurm_master_info

        - name: DEBUG | Print slurm master server info
          debug:
            var: _slurm_master_info
          when: slurm_cluster_debug | bool

        - name: Add slurm master to in-memory ansible inventory
          add_host:
            name: "{{ _slurm_master_info.openstack.name }}"
            groups:
              - slurm_cluster_all
              - slurm_master_group
            ansible_host: "{{ _slurm_master_info.openstack.accessIPv4 }}"
            ansible_user: "{{ slurm_cluster_ssh_remote_user }}"
            ansible_ssh_private_key_file: "{{ slurm_cluster_ssh_key_path }}"
            ansible_ssh_extra_args: "-F {{ slurm_cluster_ssh_cfg_path }}"
          changed_when: false

        - name: Waiting for the slurm master to be online
          delegate_to: "{{ _slurm_master_info.openstack.name }}"
          wait_for_connection:

      when: slurm_cluster_slurm_master_dedicated_machine | bool

    - name: Create a cinder volume to export by nfs
      os_volume:
        display_name: "slurm_cluster_nfs_data"
        size: "{{ slurm_cluster_nfs_server_disk_size }}"
        state: present

    - name: Attach NFS data volume to NFS server
      block:

        - name: Attach NFS volume to login (if NFS server runs in login)
          os_server_volume:
            server: "slurm_login"
            volume: "slurm_cluster_nfs_data"
            device: /dev/vdb
          when: not slurm_cluster_nfs_server_dedicated_machine

        - name: Attach NFS volume to dedicated NFS server
          os_server_volume:
            server: "slurm_nfs_server"
            volume: "slurm_cluster_nfs_data"
            device: "{{ slurm_cluster_nfs_server_disk_device }}"
          when: slurm_cluster_nfs_server_dedicated_machine


    # Left here as reference
    #
    # An alternative to build the static inventory is to use ansible module "os_server_info"
    # but this would require a specific template adapted to the output of "os_server_info"
    #
    # With the current approach template "ansible_inventory.j2" uses "hostvars" which should be
    # easier to port to a different cloud in case we need it in the future
    #
    # - name: Query information about all the cluster machines (those using the cluster ssh key)
    #   os_server_info:
    #     filters:
    #       key_name: "{{ slurm_cluster_ssh_key_name }}"
    #   environment:
    #     OS_PROJECT_DOMAIN_ID: "{{ lookup('env','OS_PROJECT_DOMAIN_ID') }}"
    #     OS_PROJECT_ID: "{{ lookup('env','OS_PROJECT_ID') }}"
    #     OS_REGION_NAME: "{{ lookup('env','OS_REGION_NAME') }}"
    #     OS_USER_DOMAIN_NAME: "{{ lookup('env','OS_USER_DOMAIN_NAME') }}"
    #     OS_PROJECT_NAME: "{{ lookup('env','OS_PROJECT_NAME') }}"
    #     OS_IDENTITY_API_VERSION: "{{ lookup('env','OS_IDENTITY_API_VERSION') }}"
    #     OS_PASSWORD: "{{ lookup('env','OS_PASSWORD') }}"
    #     OS_AUTH_URL: "{{ lookup('env','OS_AUTH_URL') }}"
    #     OS_USERNAME: "{{ lookup('env','OS_USERNAME') }}"
    #     OS_INTERFACE: "{{ lookup('env','OS_INTERFACE') }}"
    #   register: _os_server_info

    # - name: DEBUG | Print details about all the cluster machines
    #   debug:
    #     var: _os_server_info
    #   when: slurm_cluster_debug | bool

    # - name: Create the ansible inventory
    #   template:
    #     dest: inventory/hosts
    #     src: ansible_inventory.j2

- name: Create the ansible inventory
  hosts: all
  become: yes
  gather_facts: yes
  remote_user: "{{ slurm_cluster_ssh_remote_user }}"

  tasks:

    - name: Create the ansible inventory
      delegate_to: localhost
      become: no
      template:
        dest: inventory/hosts
        src: ansible_inventory.j2
