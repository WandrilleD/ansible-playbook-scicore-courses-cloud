---

## UPDATE THESE VARS TO ADAPT THEM TO YOUR OPENSTACK CLOUD
slurm_cluster_default_image: "CentOS 7 (SWITCHengines)"
slurm_cluster_default_flavor: "c1.small"
slurm_cluster_floating_ips_pool: "public"
slurm_cluster_ssh_remote_user: "centos"
## UPDATE THESE VARS TO ADAPT THEM TO YOUR OPENSTACK CLOUD

## UPDATE VARS BELOW ACCORDING TO YOUR PREFERENCE BUT IT'S NOT MANDATORY FOR INITIAL TESTING ##

# Details for the ssh key we will create in the local machine to access the cloud machines
# We will also create a ssh config file to use the login node as ssh bastion host
slurm_cluster_ssh_key_path: "{{ ansible_env.HOME }}/.ssh/slurm_cluster_cloud_id_rsa"
slurm_cluster_ssh_key_name: "slurm_cluster_cloud"  # name used to register the ssh key in openstack
slurm_cluster_ssh_cfg_path: "{{ ansible_env.HOME }}/.ssh/slurm_cluster_cloud.cfg"

# Details for the openstack security groups we will create (openstack firewall rules)
slurm_cluster_security_group_default: "slurm_cluster_default"
slurm_cluster_security_group_login: "slurm_cluster_login"

slurm_cluster_open_ports_in_login_node:
  - 22
  - 80
  - 443

# OpenStack image and flavor to use for the LOGIN NODE
# Use "openstack image list" and "openstack flavor list" to list those available in your cloud
slurm_cluster_login_image: "{{ slurm_cluster_default_image }}"
slurm_cluster_login_flavor: "{{ slurm_cluster_default_flavor }}"

# OpenStack image and flavor to use for the COMPUTE NODES
slurm_cluster_compute_image: "{{ slurm_cluster_default_image }}"
slurm_cluster_compute_flavor: "{{ slurm_cluster_default_flavor }}"

# how many slurm workers nodes to boot
slurm_cluster_num_workers: 2

# Set this to true to boot a dedicated machine for NFS server
# The default is that NFS server runs in the login node
slurm_cluster_nfs_server_dedicated_machine: false
slurm_cluster_nfs_server_image: "{{ slurm_cluster_default_image }}"
slurm_cluster_nfs_server_flavor: "{{ slurm_cluster_default_flavor }}"

# based on this we will create a volume, attach it to nfs server and export it
# size is in GB
slurm_cluster_nfs_server_disk_size: 50
slurm_cluster_nfs_server_disk_device: /dev/vdb
slurm_cluster_nfs_server_disk_mount_point: /shared
slurm_cluster_nfs_server_disk_fstype: xfs
# only define this variable if you want to specify the volume type to use for NFS export
# you can get the list of volume types with "openstack volume type list"
#slurm_cluster_nfs_server_disk_volume_type: "ceph-ssd"

# config for the NFS server
nfs_exports:
  - "{{ slurm_cluster_nfs_server_disk_mount_point }} *(rw,sync,no_root_squash)"

# config for the NFS clients
# hostname "slurm-nfs-server" is defined in /etc/hosts for every node in the cluster
nfs_share_mounts:
  - path: "{{ slurm_cluster_nfs_server_disk_mount_point }}"
    location: "slurm-nfs-server:{{ slurm_cluster_nfs_server_disk_mount_point }}"

# Set this to true to boot a dedicated machine for slurm master
# The default is that slurm master runs in the login node
slurm_cluster_slurm_master_dedicated_machine: false
slurm_cluster_slurm_master_image: "{{ slurm_cluster_default_image }}"
slurm_cluster_slurm_master_flavor: "{{ slurm_cluster_default_flavor }}"

# How to configure the mysql database for the slurm accounting daemon
slurm_cluster_slurmdbd_mysql_password: "Guan1iineN6yaxec"
slurm_cluster_slurmdbd_mysql_user: "slurmdbuser"
slurm_cluster_slurmdbd_mysql_db: "slurm"

# This RPM will be installed to provide the OpenHPC repositories. We will install slurm packages from this repo
# Check latest releases in https://openhpc.community/downloads/
slurm_cluster_ohpc_repos_url: "https://github.com/openhpc/ohpc/releases/download/v1.3.GA/ohpc-release-1.3-1.el7.x86_64.rpm"

# By default selinux is disabled in every machine using role https://galaxy.ansible.com/pescobar/selinux_disable
slurm_cluster_selinux_disabled: true
selinux_reboot: true  # should we reboot the VM when selinux is disabled?

# Set this variable to "True" in case you want to do a "yum -y update" in every machine
slurm_cluster_upgrade_all_packages: false

# list of packages we want in every machine
slurm_cluster_packages_to_install:
  - vim
  - emacs-nox
  - nano
  - git
  - htop
  - bash-completion
  - singularity
  - screen
  - tmux

# Details for the local accounts to create in every machine
slurm_cluster_users_accounts: 4
slurm_cluster_users_home_folder: "{{ slurm_cluster_nfs_server_disk_mount_point }}/home"
slurm_cluster_users_default_password: "$6$ldwSs5IPrUFC$mTJprVShXmBCd.DMYGR0SP31BcUUiReQEErn2s4cN3o91V5du/FnurZF/RU/nPnfrWZ1Sxr1pdjtvH6uG37Ee."  # slurm_cluster

# Set this var to "True" to configure the ComputeCanada software stack in every machine
slurm_cluster_enable_compute_canada_software_stack: false
# should we launch a process in background to do initial sync of the software stack to local disk?
slurm_cluster_prewarm_compute_canada_software_stack_cache: false
cvmfs_client_configure_storage: false  # never modify this option. It should be always False

# Define the list of R packages you want to install the login node (to be used in Rstudio)
rstudio_server_extra_r_packages:
  - cluster
  # - ggplot2
  # - gplots
  # - dplyr
  # - DESeq2
  # - edgeR
  # - QuasR

cgroup_memory_limit: '4G'
