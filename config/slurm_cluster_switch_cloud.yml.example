---

## UPDATE THESE VARS TO ADAPT THEM TO YOUR OPENSTACK CLOUD
slurm_cluster_default_image: "CentOS 7 (SWITCHengines)"
slurm_cluster_default_flavor: "c1.small"
slurm_cluster_floating_ips_pool: "public"
slurm_cluster_ssh_remote_user: "centos"
## UPDATE THESE VARS TO ADAPT THEM TO YOUR OPENSTACK CLOUD

## UPDATE VARS BELOW ACCORDING TO YOUR PREFERENCE BUT IT'S NOT MANDATORY FOR INITIAL TESTING ##

slurm_cluster_ssh_key_path: "{{ ansible_env.HOME }}/.ssh/slurm_cluster_cloud_id_rsa"
slurm_cluster_ssh_key_name: "slurm_cluster_cloud"  # name used to register the ssh key in openstack
slurm_cluster_ssh_cfg_path: "{{ ansible_env.HOME }}/.ssh/slurm_cluster_cloud.cfg"

slurm_cluster_security_group_default: "slurm_cluster_default"
slurm_cluster_security_group_login: "slurm_cluster_login"

slurm_cluster_open_ports_in_login_node:
  - 22
  - 80
  - 443

slurm_cluster_login_image: "{{ slurm_cluster_default_image }}"
slurm_cluster_login_flavor: "{{ slurm_cluster_default_flavor }}"

slurm_cluster_compute_image: "{{ slurm_cluster_default_image }}"
slurm_cluster_compute_flavor: "{{ slurm_cluster_default_flavor }}"
slurm_cluster_num_workers: 2

# Set this to true to boot a dedicated machine for NFS server
# The default is that NFS server is hosted in the login node
slurm_cluster_nfs_server_dedicated_machine: false
slurm_cluster_nfs_server_image: "{{ slurm_cluster_default_image }}"
slurm_cluster_nfs_server_flavor: "{{ slurm_cluster_default_flavor }}"

# based on this we will create a volume, attach it to nfs server and export it
# size is in GB
slurm_cluster_nfs_server_disk_size: 50
slurm_cluster_nfs_server_disk_device: /dev/vdb
slurm_cluster_nfs_server_disk_mount_point: /shared
slurm_cluster_nfs_server_disk_fstype: xfs

# config for the NFS server
nfs_exports:
  - "{{ slurm_cluster_nfs_server_disk_mount_point }} *(rw,sync,no_root_squash)"

# config for the NFS clients
nfs_share_mounts:
  - path: "{{ slurm_cluster_nfs_server_disk_mount_point }}"
    location: "slurm-nfs-server:{{ slurm_cluster_nfs_server_disk_mount_point }}"

# Set this to true to boot a dedicated machine for slurm master
# The default is that slurm master is hosted in the login node
slurm_cluster_slurm_master_dedicated_machine: false
slurm_cluster_slurm_master_image: "{{ slurm_cluster_default_image }}"
slurm_cluster_slurm_master_flavor: "{{ slurm_cluster_default_flavor }}"

slurm_cluster_slurmdbd_mysql_password: "Guan1iineN6yaxec"
slurm_cluster_slurmdbd_mysql_user: "slurmdbuser"
slurm_cluster_slurmdbd_mysql_db: "slurm"

slurm_cluster_ohpc_repos_url: "https://github.com/openhpc/ohpc/releases/download/v1.3.GA/ohpc-release-1.3-1.el7.x86_64.rpm"

slurm_cluster_selinux_disabled: true
selinux_reboot: true  # should we reboot the VM when selinux is disabled?

slurm_cluster_upgrade_all_packages: false

# list of packages we want in every machine
slurm_cluster_packages_to_install:
  - vim
  - emacs-nox
  - git
  - htop
  - bash-completion

slurm_cluster_users_accounts: 4
slurm_cluster_users_home_folder: "{{ slurm_cluster_nfs_server_disk_mount_point }}/home"
slurm_cluster_users_default_password: "$6$ldwSs5IPrUFC$mTJprVShXmBCd.DMYGR0SP31BcUUiReQEErn2s4cN3o91V5du/FnurZF/RU/nPnfrWZ1Sxr1pdjtvH6uG37Ee."  # slurm_cluster

slurm_cluster_enable_compute_canada_software_stack: false
cvmfs_client_configure_storage: false
